# Cursor IDE AI Rules for MidiDrumiGen v2.0

## Project Identity

You are an expert AI assistant helping develop **MidiDrumiGen v2.0**, an intelligent Max for Live device that generates authentic drum patterns in the style of any artist using on-demand research and LLM-based generation. This project uses ONLY modern, actively maintained libraries and follows the BMAD-METHOD framework.

## v2.0 Architecture (Critical Understanding)

**THIS IS NOT A TRAINING PROJECT.** v2.0 uses:
- ✅ On-demand multi-source research (web, audio, MIDI, scholarly)
- ✅ LLM-based generation (OpenAI, Claude, Gemini)
- ✅ PostgreSQL + pgvector for style profiles
- ✅ Orchestrator-Agent architecture
- ❌ NO PyTorch training
- ❌ NO pre-trained models
- ❌ NO tokenization

### ALWAYS Use These v2.0 Components
- ✅ FastAPI 0.115+ for REST API
- ✅ Celery 5.4+ for async task processing
- ✅ Redis 7.2+ for message queue
- ✅ PostgreSQL 16+ with pgvector 0.3.6+
- ✅ Anthropic 0.39+ (Primary LLM - Claude 3.5 Sonnet)
- ✅ Google GenAI 0.8+ (Secondary LLM - Gemini 2.5/3)
- ✅ OpenAI 1.54+ (Tertiary/Fallback LLM - ChatGPT 5.1)
- ✅ BeautifulSoup4 4.12+ for web scraping
- ✅ Librosa 0.10+ for audio analysis
- ✅ spaCy 3.8+ for NLP
- ✅ Sentence Transformers 3.3+ for embeddings
- ✅ mido 1.3.3 for MIDI I/O

### NEVER Use These (Removed from v1.x)
- ❌ PyTorch / TensorFlow (no training in v2.0)
- ❌ MidiTok (no tokenization needed)
- ❌ Magenta / GrooVAE / MusicVAE (legacy)
- ❌ pretty-midi (abandoned)
- ❌ Any ML training frameworks

## Code Style and Standards

### Python Version
- Target: Python 3.11+ (Recommended: 3.12)
- Use modern Python features (match/case, type hints, dataclasses, async/await)
- Follow PEP 8 with Black formatter (line length: 100)
- Use Ruff 0.8+ for linting

### Type Hints
```python
# Always use comprehensive type hints
from typing import List, Dict, Optional, Tuple
import torch
from pathlib import Path

def generate_pattern(
    style: str,
    bars: int,
    time_signature: Tuple[int, int],
    tempo: int,
    device: torch.device = torch.device("cuda")
) -> torch.Tensor:
    """Generate drum pattern with specified parameters."""
    pass
```

### Error Handling
```python
# Use specific exceptions and proper error messages
class PatternGenerationError(Exception):
    """Raised when pattern generation fails."""
    pass

try:
    pattern = model.generate(tokens)
except torch.cuda.OutOfMemoryError:
    logger.error("GPU OOM during generation, falling back to CPU")
    pattern = model.to("cpu").generate(tokens)
```

### Async Operations
```python
# Use async/await for I/O operations
from fastapi import FastAPI
import asyncio

@app.post("/generate")
async def generate_pattern(request: PatternRequest):
    task = tasks.generate_pattern.delay(request.dict())
    return {"task_id": task.id}
```

## MIDI Operations Guidelines

### Use mido for All MIDI Operations
```python
# CORRECT - Use mido
import mido
from mido import MidiFile, MidiTrack, Message

mid = MidiFile()
track = MidiTrack()
track.append(Message('note_on', note=36, velocity=80, time=0))
track.append(Message('note_off', note=36, time=480))
mid.tracks.append(track)
mid.save('pattern.mid')
```

### NEVER Import Legacy Libraries
```python
# WRONG - DO NOT USE
import magenta  # ❌ Legacy TensorFlow
import pretty_midi  # ❌ Abandoned library
from magenta.models.music_vae import TrainedModel  # ❌ Legacy
```

## LLM Provider Guidelines (v2.0 Core Feature)

### Multi-Provider Manager
```python
from openai import AsyncOpenAI
from anthropic import AsyncAnthropic
import google.generativeai as genai
from typing import Optional, Dict, Any

class LLMProviderManager:
    """Manages multiple LLM providers with automatic fallback."""
    
    def __init__(self, config: Dict[str, str]):
        self.openai_client = AsyncOpenAI(api_key=config.get("openai_key"))
        self.anthropic_client = AsyncAnthropic(api_key=config.get("anthropic_key"))
        genai.configure(api_key=config.get("google_key"))
        self.providers = ["openai", "anthropic", "google"]
    
    async def generate(
        self,
        prompt: str,
        provider: str = "openai",
        fallback: bool = True
    ) -> Dict[str, Any]:
        """Generate with automatic fallback on failure."""
        try:
            if provider == "openai":
                return await self._generate_openai(prompt)
            elif provider == "anthropic":
                return await self._generate_claude(prompt)
            elif provider == "google":
                return await self._generate_gemini(prompt)
        except Exception as e:
            if fallback and len(self.providers) > 1:
                # Try next provider
                next_provider = self._get_next_provider(provider)
                return await self.generate(prompt, next_provider, fallback=False)
            raise
```

### Prompt Engineering for Drum Generation
```python
def build_generation_prompt(
    style_profile: StyleProfile,
    params: GenerationParams
) -> str:
    """Construct detailed prompt for LLM drum generation."""
    
    prompt = f"""You are an expert drum programmer specializing in {style_profile.artist_name}'s style.

STYLE PROFILE:
- Tempo Range: {style_profile.tempo_range[0]}-{style_profile.tempo_range[1]} BPM
- Swing: {style_profile.swing_percentage}%
- Velocity Variation: {style_profile.velocity_variation}
- Ghost Note Probability: {style_profile.ghost_note_probability}
- Rhythmic Complexity: {style_profile.rhythmic_complexity}

GENERATION PARAMETERS:
- Bars: {params.bars}
- Time Signature: {params.time_signature[0]}/{params.time_signature[1]}
- Target Tempo: {params.tempo} BPM

CONTEXT:
{style_profile.textual_description}

TASK:
Generate a {params.bars}-bar drum pattern as JSON array with this structure:
[
  {{"time": 0.0, "note": 36, "velocity": 100, "duration": 0.1}},
  ...
]

MIDI Note Reference:
- 36: Kick
- 38: Snare
- 42: Closed Hi-Hat
- 46: Open Hi-Hat
- 49: Crash Cymbal

Requirements:
1. Strictly follow the style profile characteristics
2. Use realistic velocities (ghost notes: 20-40, accents: 90-127)
3. Include appropriate swing/shuffle based on style
4. Maintain consistent time signature
5. Return ONLY valid JSON (no markdown, no explanation)
"""
    return prompt
```

### Research Pipeline Integration
```python
from typing import List, Dict
import asyncio

class ResearchOrchestrator:
    """Coordinates multiple research collectors for artist analysis."""
    
    async def research_artist(self, artist_name: str) -> StyleProfile:
        """Execute parallel research across all collectors."""
        
        # Launch collectors in parallel
        tasks = [
            self.scholar_collector.collect(artist_name),
            self.midi_collector.collect(artist_name),
            self.text_collector.collect(artist_name),
            self.audio_collector.collect(artist_name),
        ]
        
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # Synthesize into StyleProfile
        profile = self._synthesize_profile(artist_name, results)
        
        # Store in database with embeddings
        await self.db.store_profile(profile)
        
        return profile
```

## FastAPI Endpoint Structure

```python
from fastapi import FastAPI, BackgroundTasks, HTTPException
from pydantic import BaseModel, Field
from typing import Literal

app = FastAPI(title="Drum Pattern Generator API")

class PatternRequest(BaseModel):
    producer_style: str = Field(..., description="Producer name for style emulation")
    bars: int = Field(4, ge=1, le=32)
    time_signature: Tuple[int, int] = (4, 4)
    tempo: int = Field(120, ge=40, le=300)
    humanize: bool = True

@app.post("/api/v1/generate")
async def generate_pattern(request: PatternRequest):
    """Generate drum pattern asynchronously."""
    # Validation
    if request.producer_style not in AVAILABLE_STYLES:
        raise HTTPException(404, f"Style '{request.producer_style}' not found")
    
    # Queue task
    task = tasks.generate_pattern.delay(request.dict())
    return {"task_id": task.id, "status": "queued"}
```

## Celery Task Definitions

```python
from celery import Celery
import torch

celery_app = Celery(
    'drum_generator',
    broker='redis://localhost:6379/0',
    backend='redis://localhost:6379/0'
)

@celery_app.task(bind=True, max_retries=3)
def generate_pattern(self, params: dict) -> dict:
    """Generate drum pattern in background."""
    try:
        # Load model
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        model = load_model(params['producer_style']).to(device)
        
        # Generate
        pattern = model.generate(**params)
        
        # Export MIDI
        midi_path = export_midi(pattern, params)
        
        return {"status": "complete", "midi_path": str(midi_path)}
    
    except Exception as exc:
        self.retry(exc=exc, countdown=60)
```

## File Organization Rules

### Import Order
```python
# 1. Standard library
import os
import sys
from pathlib import Path
from typing import List, Dict, Optional

# 2. Third-party packages
import torch
import torch.nn as nn
from fastapi import FastAPI
import mido

# 3. Local imports
from src.models.transformer import DrumPatternTransformer
from src.midi.utils import export_midi
from src.tasks.worker import celery_app
```

### Directory Structure Conventions
```
src/
├── api/           # FastAPI routes and models
├── models/        # PyTorch model definitions
├── tasks/         # Celery task definitions
├── midi/          # MIDI processing (mido-based)
├── training/      # Training scripts and data loaders
├── ableton/       # Ableton integration (MIDI/OSC)
└── utils/         # Shared utilities
```

## Testing Standards

### Unit Tests
```python
import pytest
import torch
from src.models.transformer import DrumPatternTransformer

def test_model_forward_pass():
    """Test model can perform forward pass."""
    model = DrumPatternTransformer(vocab_size=1000)
    input_ids = torch.randint(0, 1000, (2, 128))
    
    outputs = model(input_ids)
    
    assert outputs.logits.shape == (2, 128, 1000)
    assert not torch.isnan(outputs.logits).any()
```

### Integration Tests
```python
@pytest.mark.integration
async def test_pattern_generation_endpoint(client):
    """Test end-to-end pattern generation."""
    response = await client.post("/api/v1/generate", json={
        "producer_style": "J Dilla",
        "bars": 4,
        "time_signature": [4, 4],
        "tempo": 95
    })
    
    assert response.status_code == 200
    assert "task_id" in response.json()
```

## Documentation Standards

### Docstrings
```python
def humanize_pattern(
    pattern: torch.Tensor,
    timing_variance: float = 0.015,
    velocity_variance: float = 0.1
) -> torch.Tensor:
    """
    Apply humanization to drum pattern.
    
    Args:
        pattern: Tokenized drum pattern (batch_size, seq_len)
        timing_variance: Max timing offset in seconds (default: 15ms)
        velocity_variance: Max velocity variation as fraction (default: 10%)
    
    Returns:
        Humanized pattern with timing and velocity adjustments
    
    Example:
        >>> pattern = torch.tensor([[36, 42, 38, 42]])
        >>> humanized = humanize_pattern(pattern, timing_variance=0.02)
    """
    pass
```

## Context Engineering Usage

When working in Cursor IDE:

1. **Use @docs for Documentation**: `@docs architecture.md` to load architectural context
2. **Use @folder for Code**: `@folder src/models` to load model implementations
3. **Reference Specific Files**: `@file src/midi/export.py` for MIDI export operations
4. **Load Context Documents**: `@file .cursorcontext/05_ml_pipeline.md` for ML guidance

## Performance Considerations (v2.0)

### Database Query Optimization
```python
from sqlalchemy import select
from pgvector.sqlalchemy import Vector

async def find_similar_styles(
    artist_embedding: List[float],
    limit: int = 5
) -> List[StyleProfile]:
    """Find similar artists using vector search."""
    
    query = select(StyleProfile).order_by(
        StyleProfile.embedding.cosine_distance(artist_embedding)
    ).limit(limit)
    
    return await db.execute(query)
```

### Caching Strategy
```python
from functools import lru_cache
import redis

# Redis cache for frequent lookups
redis_client = redis.from_url(REDIS_URL)

async def get_style_profile_cached(artist_name: str) -> Optional[StyleProfile]:
    """Check cache before database query."""
    
    # Try Redis first
    cached = redis_client.get(f"profile:{artist_name}")
    if cached:
        return StyleProfile.parse_raw(cached)
    
    # Query database
    profile = await db.get_profile(artist_name)
    if profile:
        # Cache for 24 hours
        redis_client.setex(
            f"profile:{artist_name}",
            86400,
            profile.json()
        )
    
    return profile
```

### Async Task Processing
```python
from celery import group, chain

# Parallel research collectors
research_tasks = group(
    collect_scholar.s(artist_name),
    collect_midi.s(artist_name),
    collect_text.s(artist_name),
    collect_audio.s(artist_name),
)

# Chain: research → generate → export
pipeline = chain(
    research_tasks,
    synthesize_profile.s(),
    generate_pattern.s(params),
    export_midi.s()
)

result = pipeline.apply_async()
```

## Security Best Practices

```python
# Sanitize file paths
from pathlib import Path

def safe_path(user_input: str) -> Path:
    """Validate and sanitize file paths."""
    path = Path(user_input).resolve()
    if not path.is_relative_to(ALLOWED_DIR):
        raise ValueError("Path outside allowed directory")
    return path

# Environment variables for secrets
import os
from dotenv import load_dotenv

load_dotenv()
REDIS_URL = os.getenv("REDIS_URL", "redis://localhost:6379")
```

## Common Pitfalls to Avoid

1. **Don't mix sync and async**: Use `async def` consistently in FastAPI
2. **Don't forget device placement**: Always specify `.to(device)` for tensors
3. **Don't ignore OOM errors**: Implement fallback to CPU or smaller batches
4. **Don't hardcode paths**: Use `Path` from pathlib and environment variables
5. **Don't skip validation**: Use Pydantic models for all API inputs

## Git Workflow (BMAD-METHOD)

### Conventional Commits (REQUIRED)
```bash
# Format: <type>(<scope>): <subject>
feat(llm): add Anthropic Claude provider with fallback
fix(research): resolve timeout in audio collector
docs(api): update endpoint documentation
test(midi): add humanization unit tests
```

### Branching Strategy
```
main (production)
  └── dev (integration)
       ├── feature/research-pipeline
       ├── feature/llm-providers
       └── feature/max-for-live
```

### Quick Commit (Use Frequently!)
```bash
# Windows
.\scripts\commit.ps1

# macOS/Linux
./scripts/commit.sh

# Commit every 1-2 hours of work
# Push after each commit
```

See `docs/GIT_WORKFLOW.md` for complete workflow guide.

## Context Documents (Always Reference)

- `.cursorcontext/01_project_overview.md` - v2.0 overview
- `.cursorcontext/02_architecture.md` - Orchestrator-Agent design
- `.cursorcontext/03_dependencies.md` - ALL dependencies (verified 2025-11-17)
- `.cursorcontext/04_midi_operations.md` - MIDI processing
- `.cursorcontext/05_generation_pipeline.md` - LLM generation flow
- `docs/ORCHESTRATOR_META_PROMPT.md` - System blueprint
- `docs/PRD.md` - Product requirements
- `docs/ARCHITECTURE.md` - Technical architecture

## Remember

- This is v2.0: NO training, ONLY LLM generation
- Use async/await for all I/O operations
- Implement proper error handling with fallbacks
- Cache aggressively (Redis + database)
- Run `pytest` before committing
- Use `ruff check` for linting
- Follow conventional commit format
- Push frequently (every 1-2 hours)
- Reference `docs/` for comprehensive guides
